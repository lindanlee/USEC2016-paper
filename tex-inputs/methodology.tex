%!TEX root = ../paper.tex

\section{Methodology}

Our survey contained two main sections.
In one section, we presented participants with several scenarios---something undesirable that might happen with their wearable device---and asked them to rate their level of concern if each scenario were to happen.
This was intended to elicit their perception of the severity and impact of the risk.
In the other section, we asked participants to compare the risks and benefits of wearable technologies to better understood technologies, following the same methodology as a seminal study in risk perception by Fischhoff \etal \cite{Fischhoff}.
Our survey design is based on two prior perception studies, as we describe next.
%Finally, we collected demographic information, which included a privacy concern scale and whether participants owned any wearable devices. 

\subsection{Motivation}
\subsubsection{Smartphone Risk Scenarios}
Felt \etal previously studied the security concerns of smartphone users by conducting a large-scale online survey~\cite{Felt}. Their survey asked 3,115 smartphone users about 99 risk scenarios. Participants were asked how upset they would be if a certain action had occurred without permission. Participants rated each situation on a Likert scale ranging from ``indifferent (1)'' to ``very upset (5).''
Our methodology closely follows that study, but with different scenarios chosen to shed light on security and privacy risks of wearable devices.

\subsubsection{Technology Risk Perception}
Fischhoff \etal performed a seminal study of perceived risks with 30 widely used technologies~\cite{Fischhoff}. In their study, participants were asked to separately rate the risks and benefits for those technologies. They were told to think about all people affected by the technology, and to think about long-term vs. short-term risks and benefits. Then, the participants rated these technologies with respect to each other on a numerical scale, being instructed to rate the least risky or least beneficial technology a 10 and scaling the ratings linearly (e.g., a technology with risk rating 20 is considered twice as risky compared to a technology with a risk rating of 10).
We apply their methodology to evaluate perceived risks and benefits of several technologies related to wearable computing.
%Using this methodlogy, they were able to categorize different technologies based on whether they were high-risk/high-benefit, low-risk/low-benefit, and so forth. 

\subsection{Survey Questions}
In our survey, each participant answered 27 questions, across five different sections:   \\[-.8cm]

\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
\item 2 comprehension questions
\item 6 questions about wearable computing scenarios 
\item 2 questions about smartphone scenarios 
\item 2 risk/benefit questions 
\item 15 demographic questions \\[-.8cm]
\end{itemize}

We randomized the order participants saw sections of the survey (with the exception of the comprehension and demographic questions, which were always first and last, respectively), as well as the order of questions in each section.

\subsubsection{Comprehension Questions}
Because participants might be biased to specific companies (e.g., visceral reactions to Google Glass based on popular media stories), we based our questions on a fictitious wearable. Thus, the beginning of the survey introduced participants to the ``Cubetastic3000,'' which was the basis for all questions on wearables risks. We highlighted the capabilities of this device and described use cases. To ensure that participants had read and understood this device's capabilities, we ask them two multiple-choice comprehension questions.

\subsubsection{Wearables Scenarios}
We presented scenarios involving data capture using the Cubetastic3000 and asked them to rate how upset they would be if a particular data type (e.g., video, audio, gestures, etc.) were shared with a particular data recipient without asking first (see Figure \ref{fig:prompt}). Responses were collected on a 5-point Likert scale (from ``indifferent'' to ``very upset''), which was modeled after Felt et al.'s study of smartphone users' risk perceptions~\cite{Felt}. Our questions were of the format: 

\textit{``How would you feel if an app on your Cubetastic3000 learned <data> and shared it with <recipient>, without asking you first?''}. 

We created an initial pool of 288 questions by combining 72 data types (<data>) with 4 data recipients (<recipient>). The 4 possible data recipients were: \\[-.8cm]

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{images/prompt.pdf}
	\caption{An example of a wearable scenario question participants saw while taking the survey.}
	\label{fig:prompt}
\end{figure}

\begin{packed_item}
\item Your work contacts
\item Your friends
\item The public
\item The app's server (but didn't share it with anyone else)\\[-.8cm]
\end{packed_item}

The purpose of these questions was to determine the extent data types and data recipients play a role in upsetting participants when data is inappropriately shared. Additionally, we added 16 questions about other misbehaviors that did not follow this format, lacking either <data> or a <recipient>, but we found relevant nonetheless. An example of one of these questions was, ``\textit{How would you feel if an app on your Cubetastic3000 turned your device off, without asking you first?}'' There were a total of 304 questions in this set, from which we randomly 6 questions for each participant.

\subsubsection{Smartphone Scenarios}
\label{sec:smartphones}
We presented participants with a second set of scenarios to control for the type of device being used. These questions followed the format of the previous question set, but substituted ``smartphone'' for ``Cubetastic3000.'' Rather than using the previous pool of 288 <data> and <recipient> combinations, we selected 5 of the scenarios that Felt \etal found least and most concerning to their participants~\cite{Felt}. We randomly presented each participant with 2 of these 5 questions: \\[-.8cm]

\begin{packed_enum}
\item \textit{How would you feel if an app on your smartphone vibrated your phone without asking you first?}
\item \textit{How would you feel if an app on your smartphone connected to a Bluetooth device (like a headset) without asking you first?}
\item \textit{How would you feel if an app on your smartphone un-muted a phone call without asking you first?}
\item \textit{How would you feel if an app on your smartphone took screenshots when you were using other apps, without asking you first?}
\item \textit{How would you feel if an app on your smartphone sent premium (they cost money) calls or text messages, without asking you first?} 
\end{packed_enum}

\subsubsection{Risk and Benefit Assessment}
In addition to investigating reactions to particular scenarios, we examined broad perceptions of new technologies and how those compared to perceptions of other understood technologies. We modeled this section after a seminal risk perception study by Fischhoff \etal\cite{Fischhoff}, in which participants ranked technologies by their relative risk and benefit to society. We asked participants to perform this exercise for 4 technologies previously examined by Fischhoff {\it et al.}: handguns, motorcycles, lawnmowers, and electricity, which were chosen to span varying levels of risks and benefits.

Alongside the 4 studied technologies, we asked participants to evaluate one of 20 technologies relevant to wearables: internet, email, laptops, smartphones, smart watches, fitness trackers, Google Glass, Cubetastic3000, discrete camera, discrete microphone, facial recognition, facial detection, voice recognition, voice-based emotion detection, location tracking, speech-to-text, language detection, heart rate detection, age detection, and gender detection. We asked about familiar technologies such as the internet, general and specific wearable artifacts, and a range of new capabilities. 

%LL: we didn't pick it because they were studied by this study, so this is a bit distracting, especially since the related work is at the end instead of before this.. 
%Many of these technologies were selected from those studied by Egelman \etal\cite{Egelman2015}.

To parallel Fischhoff {\it et al.}'s risk perception study, we gave our participants a similar prompt to numerically express the perceived gross risk/gross benefit over a long period of time for all parties involved. We randomized whether they performed the ranking for risks or benefits first. The prompt is listed in Appendix \ref{sec:prompt}. The question format was as follows:

\textit{Fill in your <risk/benefit> numbers for the following:}\\[-.5cm]

\textit{Handguns}: \_\_\_\_\_\_\_ \\
\textit{Motorcycles}: \_\_\_\_\_\_\_\\
\textit{Lawnmowers}: \_\_\_\_\_\_\_\\
\textit{<Wearable Technology>}: \_\_\_\_\_\_\_\\
\textit{Electricity}: \_\_\_\_\_\_\_\\ [-.5cm]

\subsubsection{Additional Questions}
The exit portion of the survey firstly consisted of questions asking for age, gender, and education. Then, we asked participants if they owned a wearable device so we could control for prior exposure, and included an open-ended question on what would be the most likely risks associated with wearable devices. We end with the 10-question Internet Users' Information Privacy Concerns (IUIPC) index~\cite{malhotra2004internet}, so we could control for participants' general privacy attitudes.

\subsection{Focus Group}
We conducted a one-hour focus group to validate our design, gauge comprehension, and measure fatigue. The focus group began with participants taking the survey. Afterward, we asked participants to give feedback on the format and the content, noting any instructions or questions that were unclear. The focus group concluded with a discussion of possible benefits and risks of wearable devices, in order to brainstorm any additional scenarios to include. Finally, we compensated participants with \$30 in cash. We recruited all of our focus group participants from Craigslist. Of the 13 participants, 54\% were female, and ages ranged from 18 to 64 (mu = 36.1, sigma = 15.3).  Education backgrounds ranged from high school to doctorate degrees, and professions included student, artist, marketer, and court psychologist.

\subsection{Recruitment and Analysis Method}
We recruited 2,250 participants August 7th-13th 2014 via Amazon's Mechanical Turk. We restricted participants to those over 18, living in the United States, and having a successful HIT completion rate of 95\% or above. Based on incorrect responses to either of the two comprehension questions, we filtered out 366 (16\% of 2,250) participants. We filtered out an additional 99 participants (4\% of 2,250) due to incomplete responses, and one participant who was under 18, leaving us with a total sample size of 1,784. Of these, 55.10\% were male, with a median age of 29 ($\sigma$ = 10.37).

In performing our analysis in the next section, we chose to focus on the very upset rate (VUR) of each scenario.  The VUR is defined as the percentage of participants who reported a `5' on the Likert scales. 
We use the VURs rather than the average of all Likert scores for the same reasons as Felt {\it et al.}: the VUR does not presume that the ratings, ranging from ``indifferent'' to ``very upset,'' are linearly spaced. Additionally, most were be upset, at least a little, in all scenarios when a device takes action without permission (rating distribution: ``1''= 455, ``2'' = 523, ``3'' = 902, ``4''' = 1,746, ``5'' = 6,654). Thus, the main distinguishing factor of a participant reacting to a given scenario is whether they were maximally upset or not, rather than how upset they were.

We followed Fischhoff {\it et al.}'s methodology and did not normalize the numerical responses. Rather, we report medians and quartiles, which are not impacted by outliers. For the open-ended question at the end (i.e., additional privacy concerns), two researchers independently coded 1,784 responses, with an initial agreement rate of 89.7\%. The researchers discussed and resolved any disagreements so that the final codings reflect unanimous agreement.