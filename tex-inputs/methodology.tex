%!TEX root = ../paper.tex

\section{Methodology}
To obtain a comprehensive list of possible risks that wearable devices might present in the future, we examined the sensors, capabilities, permissions, and applications of the most popular wearable devices on the market at the time of this study. At the time of this study, August 2014, the most popular wearable devices included the Fitbit fitness tracker which performs continuously monitors heartbeat, steps taken, and sleep patterns \cite{6_fitbit_2014, 7_time_2014}, the Pebble smartwatch which can take pictures, send texts, show notifications from online, and push notifications to services \cite{pebble_smartwatch_2014, 9_verge_2014, 10_readwrite_2014}, and Google Glass \cite{11_wikipedia_2015, 12_turi_2014}. We used these wearable devices, along with other comparable wearable devices on the market, as inspiration to develop a list of security and privacy risks that users might be concerned about.

We then designed a survey to gauge how severe our participants consider these risks to be.
Our survey contained two main sections.
In one section, we presented participants with several scenarios---something undesirable that might happen with their wearable device---and asked them to rate their level of concern if each scenario were to happen.
This was intended to elicit their perception of the severity and impact of the risk.
The format of this section was based on Felt {\it et al.}'s study of user perceptions of security and privacy risks with mobile devices.
In the other section, we asked participants to compare the risks and benefits of wearable technologies to those of better-understood technologies, following the same methodology from Fischhoff {\it et al.}'s seminal study in risk perception~\cite{Fischhoff}.
%Finally, we collected demographic information, which included a privacy concern scale and whether participants owned any wearable devices. 

\subsection{Motivation}
\subsubsection{Smartphone Risk Scenarios}
Felt \etal previously studied the security concerns of smartphone users by conducting a large-scale online survey~\cite{Felt}. Their survey asked 3,115 smartphone users about 99 risk scenarios. Participants were asked how upset they would be if a certain action occurred without their permission. Participants rated each situation on a Likert scale ranging from ``indifferent (1)'' to ``very upset (5).''
Our methodology closely follows that study, but with scenarios chosen to shed light on the security and privacy risks of wearable devices.

\subsubsection{Technology Risk Perception}
Fischhoff \etal performed a seminal study of the perceived risks of 30 widely used technologies~\cite{Fischhoff}. In their study, participants were asked to separately rate the risks and benefits for these technologies. They were told to think about all people affected by the technology, and to think about long-term vs. short-term risks and benefits. Then, the participants rated these technologies with respect to each other on a numerical scale, being instructed to rate the least risky or least beneficial technology a 10 and scaling the ratings linearly (e.g., a technology with risk rating 20 is considered twice as risky compared to a technology with a risk rating of 10).
We apply their methodology to evaluate the perceived risks and benefits of several technologies related to wearable computing.
%Using this methodlogy, they were able to categorize different technologies based on whether they were high-risk/high-benefit, low-risk/low-benefit, and so forth. 

\subsection{Survey Questions}
In our survey, each participant answered 27 questions, across five different sections:   \\[-.8cm]

\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
\item 2 reading comprehension questions
\item 6 questions about wearable computing scenarios 
\item 2 questions about smartphone scenarios 
\item 2 Fischhoff-style risk/benefit questions 
\item 15 demographic questions \\[-.8cm]
\end{itemize}

We randomized the order participants saw sections of the survey (with the exception of the comprehension and demographic questions, which were always first and last, respectively), as well as the order of questions in each section.

\subsubsection{Comprehension Questions}
Because participants might be biased to specific companies (e.g., visceral reactions to Google Glass based on popular media stories), we based our questions on a fictitious wearable. Thus, the beginning of the survey introduced participants to the ``Cubetastic3000,'' which was the basis for all questions on wearables risks. We highlighted the capabilities of this device and described use cases. To ensure that participants had read and understood this device's capabilities, we ask them two multiple-choice comprehension questions.

\subsubsection{Wearable Scenarios}
We presented scenarios involving data captured by the Cubetastic3000 and asked participants to rate how upset they would be if a particular type of data (e.g., video, audio, gestures, etc.) were shared with a particular data recipient without asking them first (see Figure \ref{fig:prompt}). Responses were collected on a 5-point Likert scale (from ``indifferent'' to ``very upset''), following Felt \etal\cite{Felt}. Questions were of the form: 

\textit{``How would you feel if an app on your Cubetastic3000 learned <data> and shared it with <recipient>, without asking you first?''}. 

We created an initial pool of 288 questions by combining 72 data types (<data>) with 4 data recipients (<recipient>). The 4 possible data recipients were: \\[-.8cm]

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{images/prompt.pdf}
	\caption{An example of a wearable scenario question participants saw while taking the survey.}
	\label{fig:prompt}
\end{figure}

\begin{packed_item}
\item Your work contacts
\item Your friends
\item The public
\item The app's server (but didn't share it with anyone else)\\[-.8cm]
\end{packed_item}

The purpose of these questions was to determine how upset participants would be when data is inappropriately shared, and to understand how their reaction depends upon the type and recipient of the data that is shared.
There were a total of 288 questions in this set, from which we randomly selected 6 questions for each participant.

\subsubsection{Smartphone Scenarios}
\label{sec:smartphones}
We presented participants with a second set of scenarios to control for the type of device being used. Rather than using the previous pool of 288 <data> and <recipient> combinations, we selected 5 of the scenarios that Felt \etal found least and most concerning to their participants~\cite{Felt}. We randomly presented each participant with 2 of these 5 questions: 

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item How would you feel if an app on your <device> vibrated your phone without asking you first?
\item How would you feel if an app on your <device> connected to a Bluetooth device (like a headset) without asking you first?
\item How would you feel if an app on your <device> un-muted a phone call without asking you first?
\item How would you feel if an app on your <device> took screenshots when you were using other apps, without asking you first?
\item How would you feel if an app on your <device> sent premium (they cost money) calls or text messages, without asking you first?
\end{enumerate}

So that we could perform controlled comparisons based on device type, we also included a version of each of these questions in the wearable scenarios section that substituted ``Cubetastic3000'' for ``smartphone.'' Thus, there were a total of 293 Cubetastic3000 scenarios, from which each participant was randomly assigned 6.

\subsubsection{Risk and Benefit Assessment}
In addition to investigating reactions to particular scenarios, we examined broad perceptions of new technologies and how those compared to perceptions of other understood technologies. We modeled this section after a seminal risk perception study by Fischhoff \etal\cite{Fischhoff}, in which participants ranked technologies by their relative risk and benefit to society. We asked participants to perform this exercise for 4 technologies previously examined by Fischhoff {\it et al.}: handguns, motorcycles, lawnmowers, and electricity, which were chosen to span varying levels of risks and benefits.

Alongside the 4 studied technologies, we asked participants to evaluate one of 20 technologies relevant to wearables: internet, email, laptops, smartphones, smart watches, fitness trackers, Google Glass, Cubetastic3000, discrete camera, discrete microphone, facial recognition, facial detection, voice recognition, voice-based emotion detection, location tracking, speech-to-text, language detection, heart rate detection, age detection, and gender detection. We asked about familiar technologies such as the internet, general and specific wearable artifacts, and a range of new capabilities. 

%LL: we didn't pick it because they were studied by this study, so this is a bit distracting, especially since the related work is at the end instead of before this.. 
%Many of these technologies were selected from those studied by Egelman \etal\cite{Egelman2015}.

To parallel Fischhoff {\it et al.}'s risk perception study, we gave our participants a similar prompt to numerically express the perceived gross risk/gross benefit over a long period of time for all parties involved. We randomized whether they performed the ranking for risks or benefits first. The prompt is listed in Appendix \ref{sec:prompt}. The question format was as follows:

\textit{Fill in your <risk/benefit> numbers for the following:}\\[-.5cm]

\textit{Handguns}: \_\_\_\_\_\_\_ \\
\textit{Motorcycles}: \_\_\_\_\_\_\_\\
\textit{Lawnmowers}: \_\_\_\_\_\_\_\\
\textit{<Wearable Technology>}: \_\_\_\_\_\_\_\\
\textit{Electricity}: \_\_\_\_\_\_\_\\ [-.5cm]

\subsubsection{Additional Questions}
The exit portion of the survey firstly consisted of questions asking for age, gender, and education. Then, we asked participants if they owned a wearable device so we could control for prior exposure, and included an open-ended question on what would be the most likely risks associated with wearable devices. We end with the 10-question Internet Users' Information Privacy Concerns (IUIPC) index~\cite{malhotra2004internet}, so we could control for participants' general privacy attitudes.

\subsection{Focus Group}
We conducted a one-hour focus group to validate our design, gauge comprehension, and measure fatigue. The focus group began with participants taking the survey. Afterward, we asked participants to give feedback on the format and the content, noting any instructions or questions that were unclear. The focus group concluded with a discussion of possible benefits and risks of wearable devices, in order to brainstorm any additional scenarios to include. Finally, we compensated participants with \$30 in cash. We recruited all of our focus group participants from Craigslist. Of the 13 participants, 54\% were female, and ages ranged from 18 to 64 (mu = 36.1, sigma = 15.3).  Education backgrounds ranged from high school to doctorate degrees, and professions included student, artist, marketer, and court psychologist.

\subsection{Recruitment and Analysis Method}
We recruited 2,250 participants August 7th-13th 2014 via Amazon's Mechanical Turk. We restricted participants to those over 18, living in the United States, and having a successful HIT completion rate of 95\% or above. Based on incorrect responses to either of the two comprehension questions, we filtered out 366 (16\% of 2,250) participants. We filtered out an additional 99 participants (4\% of 2,250) due to incomplete responses, and three participants who were under 18, leaving us with a total sample size of 1,782. Of these, 57.9\% were male (1,031), 41.0\% were female (731), and 20 participants declined to state their genders. Ages ranged from 18 to 73, with a mean of 32.1 ($\sigma$ = 10.37). Almost half of our participants had completed a college degree or more (49.2\% of 1,782), which includes the 219 (12.3\% of 1,782) who reported graduate degrees. While our sample was younger and more educated than the U.S. population as a whole, we believe it is still consistent with the U.S. Internet-using population.

In performing our analysis in the next section, we chose to focus on the very upset rate (VUR) of each scenario.  The VUR is defined as the percentage of participants who reported a `5' on the Likert scales. 
We use the VURs rather than the average of all Likert scores for the same reasons as Felt {\it et al.}: the VUR does not presume that the ratings, ranging from ``indifferent'' to ``very upset,'' are linearly spaced. Additionally, most were be upset, at least a little, in all scenarios when a device takes action without permission (rating distribution: ``1''= 759, ``2'' = 918, ``3'' = 1,452, ``4''' = 2,421, ``5'' = 8,344). Thus, the main distinguishing factor of a participant reacting to a given scenario is whether they were maximally upset or not, rather than how upset they were.

We followed Fischhoff {\it et al.}'s methodology and did not normalize the numerical responses. Rather, we report medians and quartiles, which are not impacted by outliers. For the open-ended question at the end (i.e., additional privacy concerns), two researchers independently coded 1,782 responses, with an initial agreement rate of 89.7\%. The researchers discussed and resolved any disagreements so that the final codings reflect unanimous agreement.
